import AVFoundation
import UIKit

/// è¯­éŸ³æ’­æŠ¥å·¥å…·ç±»ï¼Œä¸“ä¸ºæ— éšœç¢ç”¨æˆ·è®¾è®¡
class SpeechHelper: NSObject, @unchecked Sendable {
    
    /// å•ä¾‹å®ä¾‹
    static let shared = SpeechHelper()
    
    private let synthesizer = AVSpeechSynthesizer()
    private var isCurrentlySpeaking = false
    
    private override init() {
        super.init()
        synthesizer.delegate = self
        
        // é…ç½®éŸ³é¢‘ä¼šè¯ - ç®€åŒ–é…ç½®é¿å…å‚æ•°é”™è¯¯
        configureAudioSession()
        
        // è®¾ç½®éŸ³é¢‘ä¸­æ–­ç›‘å¬
        setupAudioInterruptionHandling()
        
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(appWillEnterForeground),
            name: UIApplication.willEnterForegroundNotification,
            object: nil
        )
    }
    
    @objc private func appWillEnterForeground() {
        configureAudioSession()
    }
    
    /// è®¾ç½®éŸ³é¢‘ä¸­æ–­å¤„ç†
    private func setupAudioInterruptionHandling() {
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleAudioInterruption),
            name: AVAudioSession.interruptionNotification,
            object: nil
        )
        
        NotificationCenter.default.addObserver(
            self,
            selector: #selector(handleRouteChange),
            name: AVAudioSession.routeChangeNotification,
            object: nil
        )
    }
    
    /// å¤„ç†éŸ³é¢‘ä¸­æ–­ï¼ˆæ¥ç”µã€å…¶ä»–åº”ç”¨ç­‰ï¼‰
    @objc private func handleAudioInterruption(_ notification: Notification) {
        guard let info = notification.userInfo,
              let typeValue = info[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }
        
        switch type {
        case .began:
            print("ğŸ”‡ éŸ³é¢‘ä¼šè¯è¢«ä¸­æ–­ï¼ˆæ¥ç”µã€å…¶ä»–åº”ç”¨ç­‰ï¼‰")
            // ä¸­æ–­å¼€å§‹æ—¶æš‚åœè¯­éŸ³æ’­æŠ¥
            if isCurrentlySpeaking {
                synthesizer.pauseSpeaking(at: .immediate)
                print("æš‚åœå½“å‰è¯­éŸ³æ’­æŠ¥")
            }
            
        case .ended:
            print("ğŸ”Š éŸ³é¢‘ä¸­æ–­ç»“æŸï¼Œå‡†å¤‡æ¢å¤")
            
            // æ£€æŸ¥ä¸­æ–­ç»“æŸçš„é€‰é¡¹
            if let optionsValue = info[AVAudioSessionInterruptionOptionKey] as? UInt {
                let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
                
                if options.contains(.shouldResume) {
                    print("ç³»ç»Ÿå»ºè®®æ¢å¤éŸ³é¢‘æ’­æ”¾")
                    
                    // é‡æ–°é…ç½®å’Œæ¿€æ´»éŸ³é¢‘ä¼šè¯
                    DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
                        self.configureAudioSession()
                        
                        // æ¢å¤è¯­éŸ³æ’­æŠ¥
                        if self.synthesizer.isPaused {
                            self.synthesizer.continueSpeaking()
                            print("æ¢å¤è¯­éŸ³æ’­æŠ¥")
                        }
                    }
                } else {
                    print("ç³»ç»Ÿä¸å»ºè®®è‡ªåŠ¨æ¢å¤ï¼Œç”¨æˆ·éœ€æ‰‹åŠ¨æ“ä½œ")
                }
            }
            
        @unknown default:
            print("æœªçŸ¥çš„éŸ³é¢‘ä¸­æ–­ç±»å‹")
        }
    }
    
    /// å¤„ç†éŸ³é¢‘è·¯ç”±å˜åŒ–ï¼ˆè“ç‰™è¿æ¥/æ–­å¼€ã€è€³æœºæ’æ‹”ç­‰ï¼‰
    @objc private func handleRouteChange(_ notification: Notification) {
        guard let info = notification.userInfo,
              let reasonValue = info[AVAudioSessionRouteChangeReasonKey] as? UInt,
              let reason = AVAudioSession.RouteChangeReason(rawValue: reasonValue) else {
            return
        }
        
        switch reason {
        case .newDeviceAvailable:
            print("ğŸ§ æ–°éŸ³é¢‘è®¾å¤‡å¯ç”¨ï¼ˆè“ç‰™ã€è€³æœºç­‰ï¼‰")
            
        case .oldDeviceUnavailable:
            print("ğŸ§ éŸ³é¢‘è®¾å¤‡æ–­å¼€")
            // è®¾å¤‡æ–­å¼€æ—¶æš‚åœæ’­æŠ¥ï¼Œé¿å…åˆ‡æ¢åˆ°æ‰¬å£°å™¨æ—¶éŸ³é‡è¿‡å¤§
            if isCurrentlySpeaking {
                synthesizer.pauseSpeaking(at: .immediate)
                print("å› è®¾å¤‡æ–­å¼€æš‚åœè¯­éŸ³æ’­æŠ¥")
            }
            
        case .categoryChange:
            print("ğŸ”„ éŸ³é¢‘ç±»åˆ«å‘ç”Ÿå˜åŒ–")
            // é‡æ–°é…ç½®éŸ³é¢‘ä¼šè¯ä»¥ç¡®ä¿è®¾ç½®æ­£ç¡®
            configureAudioSession()
            
        case .override:
            print("ğŸ”„ éŸ³é¢‘ä¼šè¯è¢«ç³»ç»Ÿè¦†ç›–")
            
        case .wakeFromSleep:
            print("ğŸŒ… ä»ç¡çœ ä¸­å”¤é†’")
            configureAudioSession()
            
        case .noSuitableRouteForCategory:
            print("âš ï¸ å½“å‰ç±»åˆ«æ²¡æœ‰åˆé€‚çš„éŸ³é¢‘è·¯ç”±")
            
        case .routeConfigurationChange:
            print("ğŸ”„ éŸ³é¢‘è·¯ç”±é…ç½®å˜åŒ–")
            
        case .unknown:
            print("âš ï¸ æœªçŸ¥åŸå› çš„éŸ³é¢‘è·¯ç”±å˜åŒ–")
            
        @unknown default:
            print("æœªçŸ¥çš„éŸ³é¢‘è·¯ç”±å˜åŒ–åŸå› ")
        }
    }
    
    /// æ£€æµ‹è®¾å¤‡æ˜¯å¦å¤„äºé™éŸ³çŠ¶æ€
    private func isSilentModeEnabled() -> Bool {
        // é€šè¿‡éŸ³é¢‘ä¼šè¯æ£€æµ‹é™éŸ³çŠ¶æ€
        let audioSession = AVAudioSession.sharedInstance()
        
        // æ£€æŸ¥éŸ³é¢‘ä¼šè¯çš„è¾“å‡ºéŸ³é‡
        if audioSession.outputVolume == 0.0 {
            return true
        }
        
        // æ£€æŸ¥ä¸­æ–­çŠ¶æ€ï¼ˆå¯èƒ½ç”±äºé™éŸ³å¼€å…³å¯¼è‡´ï¼‰
        if audioSession.secondaryAudioShouldBeSilencedHint {
            return true
        }
        
        return false
    }
    
    /// æ’­æŠ¥æ–‡æœ¬å†…å®¹
    /// - Parameter text: è¦æ’­æŠ¥çš„æ–‡æœ¬
    /// - Parameter rate: è¯­é€Ÿï¼Œé»˜è®¤ä¸ºæ­£å¸¸é€Ÿåº¦
    /// - Parameter volume: éŸ³é‡ï¼Œé»˜è®¤ä¸º1.0
    func speak(_ text: String, rate: Float = AVSpeechUtteranceDefaultSpeechRate, volume: Float = 1.0) {
        // æ£€æŸ¥é™éŸ³çŠ¶æ€
        if isSilentModeEnabled() {
            print("è®¾å¤‡å¤„äºé™éŸ³çŠ¶æ€ï¼Œè·³è¿‡è¯­éŸ³æ’­æŠ¥: \(text)")
            return
        }
        
        // å¦‚æœæ­£åœ¨æ’­æŠ¥ï¼Œå…ˆåœæ­¢å½“å‰æ’­æŠ¥
        if isCurrentlySpeaking {
            synthesizer.stopSpeaking(at: .immediate)
        }
        
        // æ¯æ¬¡æ’­æŠ¥éƒ½æ¿€æ´»éŸ³é¢‘ä¼šè¯ï¼Œç¡®ä¿æœ¬appéŸ³é¢‘ä¼˜å…ˆ
        let audioSession = AVAudioSession.sharedInstance()
        do {
            try audioSession.setActive(true, options: [.notifyOthersOnDeactivation])
            print("éŸ³é¢‘ä¼šè¯é‡æ–°æ¿€æ´»æˆåŠŸ")
        } catch {
            print("é‡æ–°æ¿€æ´»éŸ³é¢‘ä¼šè¯å¤±è´¥: \(error.localizedDescription)")
            // å³ä½¿æ¿€æ´»å¤±è´¥ä¹Ÿç»§ç»­å°è¯•æ’­æŠ¥
        }
        
        let utterance = AVSpeechUtterance(string: text)
        utterance.rate = rate
        utterance.volume = volume
        utterance.voice = AVSpeechSynthesisVoice(language: "zh-CN")
        
        print("å¼€å§‹è¯­éŸ³æ’­æŠ¥: \(text)")
        synthesizer.speak(utterance)
    }
    
    /// æ’­æŠ¥å½“å‰æ—¶é—´
    func speakCurrentTime() {
        let now = Date()
        let calendar = Calendar.current
        let hour = calendar.component(.hour, from: now)
        let minute = calendar.component(.minute, from: now)
        
        // æ ¹æ®å°æ—¶ç¡®å®šæ—¶é—´æ®µ
        let period: String
        switch hour {
        case 0..<6:
            period = "å‡Œæ™¨"     // 0:00-5:59
        case 6..<12:
            period = "ä¸Šåˆ"     // 6:00-11:59
        case 12..<18:
            period = "ä¸‹åˆ"     // 12:00-17:59
        case 18..<24:
            period = "æ™šä¸Š"     // 18:00-23:59
        default:
            period = "å‡Œæ™¨"     // é»˜è®¤å€¼ï¼ˆä¸åº”è¯¥è¾¾åˆ°ï¼‰
        }
        
        // æ ¼å¼åŒ–æ—¶é—´æ’­æŠ¥ï¼šå½“å‰æ—¶é—´ä¸‹åˆ14ç‚¹43åˆ†
        let timeString = "å½“å‰æ—¶é—´\(period)\(hour)ç‚¹\(minute)åˆ†"
        speak(timeString)
    }
    
    /// æ’­æŠ¥è®¡æ—¶å™¨è®¾ç½®
    /// - Parameters:
    ///   - duration: è®¡æ—¶æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
    ///   - interval: æé†’é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    func speakTimerSettings(duration: Int, interval: Int) {
        let text = "è®¡æ—¶æ—¶é•¿\(duration)åˆ†é’Ÿï¼Œæé†’é—´éš”\(interval)åˆ†é’Ÿ"
        speak(text)
    }
    
    /// æ’­æŠ¥å‰©ä½™æ—¶é—´
    /// - Parameter remainingSeconds: å‰©ä½™ç§’æ•°
    func speakRemainingTime(remainingSeconds: Int) {
        let hours = remainingSeconds / 3600
        let remainingSecondsAfterHours = remainingSeconds % 3600
        // å‘ä¸Šå–æ•´åˆ†é’Ÿæ•°ï¼Œå¦‚æœæœ‰ä»»ä½•ç§’æ•°éƒ½ç®—ä½œ1åˆ†é’Ÿ
        let minutes = (remainingSecondsAfterHours + 59) / 60
        
        var text = "å‰©ä½™æ—¶é—´"
        if hours > 0 {
            text += "\(hours)å°æ—¶"
        }
        if minutes > 0 || hours == 0 {
            text += "\(minutes)åˆ†é’Ÿ"
        }
        
        speak(text)
    }
    
    /// æ’­æŠ¥è®¡æ—¶å™¨çŠ¶æ€å˜åŒ–
    /// - Parameter action: æ“ä½œç±»å‹ï¼ˆå¼€å§‹ã€æš‚åœã€æ¢å¤ã€ç»“æŸï¼‰
    func speakTimerAction(_ action: String) {
        speak(action)
    }
    
    /// æ’­æŠ¥è¯­éŸ³è¯†åˆ«ç»“æœåé¦ˆ
    /// - Parameter feedback: åé¦ˆå†…å®¹
    func speakVoiceRecognitionFeedback(_ feedback: String) {
        speak(feedback)
    }
    
    /// åœæ­¢å½“å‰æ’­æŠ¥
    func stopSpeaking() {
        if isCurrentlySpeaking {
            synthesizer.stopSpeaking(at: .immediate)
        }
    }
    
    // MARK: - Private Methods
    
    /// é…ç½®éŸ³é¢‘ä¼šè¯
    private func configureAudioSession() {
        do {
            let audioSession = AVAudioSession.sharedInstance()
            
            // é…ç½®æœ€é«˜ä¼˜å…ˆçº§çš„éŸ³é¢‘ä¼šè¯ - ä¸ä¸å…¶ä»–åº”ç”¨æ··åˆï¼Œç‹¬å éŸ³é¢‘
            if #available(iOS 16.0, *) {
                // iOS 16+ ä½¿ç”¨spoken audioæ¨¡å¼ï¼Œç‹¬å éŸ³é¢‘ä¼šè¯
                do {
                    try audioSession.setCategory(.playback, mode: .spokenAudio, options: [.duckOthers, .interruptSpokenAudioAndMixWithOthers, .allowBluetooth, .allowBluetoothA2DP, .allowAirPlay])
                    print("ä½¿ç”¨æœ€é«˜ä¼˜å…ˆçº§ spokenAudio æ¨¡å¼é…ç½®æˆåŠŸ")
                } catch {
                    // å¦‚æœspoken audioæ¨¡å¼å¤±è´¥ï¼Œä½¿ç”¨ç‹¬å æ’­æ”¾æ¨¡å¼
                    try audioSession.setCategory(.playback, options: [.duckOthers, .allowBluetooth, .allowBluetoothA2DP, .allowAirPlay])
                    print("é™çº§ä½¿ç”¨ç‹¬å  playback æ¨¡å¼é…ç½®")
                }
            } else {
                // iOS 15.6+ ä½¿ç”¨ç‹¬å æ’­æ”¾æ¨¡å¼
                try audioSession.setCategory(.playback, options: [.duckOthers, .allowBluetooth, .allowBluetoothA2DP, .allowAirPlay])
                print("ä½¿ç”¨ç‹¬å  playback æ¨¡å¼é…ç½®ï¼ˆiOS 15.6+ï¼‰")
            }
            
            // ç«‹å³æ¿€æ´»éŸ³é¢‘ä¼šè¯ï¼Œä½¿ç”¨ notifyOthersOnDeactivation ç¡®ä¿å…¶ä»–åº”ç”¨èƒ½æ¢å¤
            try audioSession.setActive(true, options: [.notifyOthersOnDeactivation])
            print("éŸ³é¢‘ä¼šè¯é…ç½®æˆåŠŸ - ç‹¬å æ¨¡å¼ï¼Œæœ€é«˜ä¼˜å…ˆçº§åå°æ’­æŠ¥")
            
        } catch {
            print("éŸ³é¢‘ä¼šè¯é…ç½®å¤±è´¥: \(error.localizedDescription)")
            // ç»§ç»­åˆå§‹åŒ–ï¼Œè¯­éŸ³åˆæˆå¯èƒ½ä»ç„¶å·¥ä½œ
        }
    }
}

// MARK: - AVSpeechSynthesizerDelegate
extension SpeechHelper: AVSpeechSynthesizerDelegate {
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didStart utterance: AVSpeechUtterance) {
        isCurrentlySpeaking = true
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        isCurrentlySpeaking = false
    }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didCancel utterance: AVSpeechUtterance) {
        isCurrentlySpeaking = false
    }
}